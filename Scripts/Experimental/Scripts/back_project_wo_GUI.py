import os
import glob
import time
import shutil

import numpy as np
from skimage.io import imread, imsave

from os.path import dirname
from functools import partial
from multiprocessing.pool import ThreadPool as TPool

import Metashape as M
M.License().activate(os.getenv('METASHAPE_LICENSE'))

# -----------------------------------------------------------------------------
# Functions
# -----------------------------------------------------------------------------


def make_depth_maps(depth_dir):
    """This function uses the Metashape Task function to create the depth
    maps, which are used to help identify if a label is obstructed from
    view."""

    camera_names = [c.label for c in doc.chunk.cameras]
    depth_maps = [doc.chunk.depth_maps[c] for c in doc.chunk.cameras]
    depths = [doc.chunk.depth_maps[c].image() for c in doc.chunk.cameras]

    depth_maps = dict(zip(camera_names, depth_maps))
    depths = dict(zip(camera_names, depths))

    print("Depth Maps Made.")

    return depth_maps, depths


def make_masks(mask_dir):
    """This function uses the Metashape Task function to create the image
    masks, which are used as the base of sparse and dense segmentation
    masks."""

    task = M.Tasks.ExportMasks(compression=None)
    task.cameras = [c.key for c in doc.chunk.cameras]
    task.path = mask_dir + "{filename}.tif"

    masks_path = glob.glob(mask_dir + "*.tif")

    print("Masks Made.")

    return masks_path


def make_sparse_label_folders(src, dst):
    """This function creates folders to contain the temporary annotation files
    for each source image."""

    images = glob.glob(src + "*.tiff")

    label_paths = []

    for image in images:
        image_basename = os.path.basename(image).split(".tiff")[0]
        os.makedirs(dst + image_basename, exist_ok=True)
        output_path = dst + image_basename + "/" + image_basename + "_"

        label_paths.append([image_basename, output_path])

    print("Sparse Folders Made.")

    return label_paths


def save_annotations(sparse_path, annotations):
    """This function takes in a numpy array containing all annotation
    information. Give the name of the source image, all linked images
    associated will be extracted and saved as a temporary npy file within a
    folder with the same name as the source image."""

    # Go through each image, find the labels that belong to it; save.
    basename, label_path = sparse_path
    annotation_path = label_path + str(int(time.time()))

    # Find the image that has a label, remove duplicate rows
    annotation = annotations[np.where(annotations[:, 0] == basename)]

    # Save as a numpy array for now
    if len(annotation) > 0:
        np.save(annotation_path, annotation)

    return annotation_path


def mt_save_annotations(annotations):
    """Multithread function handler for saving temporary numpy files"""

    p = TPool(CORES)
    annotation_paths = p.map(partial(save_annotations,
                                     annotations=annotations), sparse_paths)
    p.close()

    return annotation_paths


def construct_sparse_mask(sparse_folder, mask_dir):
    """This function takes in a folder, globs the npy files, and then
    creates a sparse segmentation mask from them. The segmentation mask is
    stored in the parent folder."""

    # Obtaining the name of the image, and setting output file path
    basename = sparse_folder.split("\\")[-2]
    mask_path = mask_dir + basename + ".tif"
    sparse_path = dirname(dirname(sparse_folder)) + "/" + basename + ".titf"

    # Globbing all npy files for this folder / source image
    npys = glob.glob(sparse_folder + "*.npy")

    # If there aren't any in there, delete the folder and move on.
    if len(npys) <= 0:
        shutil.rmtree(sparse_folder)
        return

    # Open mask generated by Metashape: black is background (no labels)
    mask = imread(mask_path, as_gray=True).astype('uint8')
    mask = np.dstack((mask, mask, mask))

    # Loop through each npy file that exists in folder, load it, and set the
    # label (RGB values) for the corresponding pixel indices
    for npy in npys:
        arr = np.load(npy, allow_pickle=True)
        arr = arr[:, 1:].astype(int)
        mask[arr[:, 1], arr[:, 0], :] = arr[:, 2:]

    # Save the sparse mask to output file path
    imsave(sparse_path, mask)

    # Make sure it was saved, then delete the folder with temporary npy files.
    if os.path.exists(sparse_path):
        shutil.rmtree(sparse_folder)

    return sparse_path


def mt_construct_sparse_mask(sparse_dir, mask_dir):
    """Multithread function handler for reconstructing sparse masks"""

    sparse_folders = glob.glob(sparse_dir + "*/")
    mask_paths = make_masks(mask_dir)

    p = TPool(CORES)
    sparse_masks = p.map(partial(construct_sparse_mask, mask_dir=mask_dir),
                         sparse_folders)
    p.close()

    return sparse_masks


def find_linked_images(indices, m_dict):
    """This function takes in a point (p) on the orthomosaic, and returns a
    list of dictionaries each containing the name of the camera that the
    point exists in, and the corresponding pixel location (u, v)."""

    chunk = m_dict['chunk']
    ortho = m_dict['ortho']
    dem = m_dict['dem']
    tiled = m_dict['tiled']
    depth_maps = m_dict['depth_maps']
    depths = m_dict['depths']
    T = m_dict['T']

    linked_cameras = []

    for index in indices:

        y, x, l = index[0], index[1], index[2:]
        R, G, B = l.tolist()

        # Converting point (p) from orthomosaic coordinates to X, Y, Z in local
        # coordinates. This makes the assumption that the orthomosaic and
        # DEM share the same coordinate system.
        X = ortho.left + ortho.resolution * x
        Y = ortho.top - ortho.resolution * y
        Z = dem.altitude(M.Vector((X, Y)))
        P = M.Vector((X, Y, Z))

        # Converting point (p) from 3D local coordinate system to internal
        # coordinate system. If the crs of the orthomosaic was in a local
        # coordinate system
        if ortho.crs.name[0:17] == 'Local Coordinates':
            p = T.inv().mulp(ortho.projection.matrix.inv().mulp(P))

        # Else, it was in a projected coordinate system (UTM)
        else:
            p = T.inv().mulp(ortho.crs.unproject(P))

        # Go to the next index early if this index isn't in the scene
        if p is None:
            continue

        # Loop through each of the cameras within the project and find those
        # that contain the point (p); store those in a list of dicts.
        for cam in chunk.cameras:

            # point (p) is in not in this camera, skip
            if not cam.project(p):
                continue

            # u (x) and v (y) pixel coordinates in camera coordinate space
            # for point p.u and v are floating point (sub-pixel): find nearest
            # whole pixel
            u = int(cam.project(p).x)
            v = int(cam.project(p).y)

            # If the pixel coordinates that the point (p) is supposed to be
            # located within the image exceeds the dimensions of the image,
            # then it isn't in the image...
            if u < 0 or u >= WIDTH or v < 0 or v >= HEIGHT:
                continue

            # Okay, so point p is "in" the image, but is it in the foreground
            # or background (obstructed from current view)? Let's find out:
            # Start by accessing this camera's depth map
            depth = depths[cam.label]
            depth_map = depth_maps[cam.label]

            # Project that point back into the scene. Q/q will represent the
            # actual point (foreground) whereas P/p might be foreground or
            # it might be background.
            Q = M.Vector([u, v])
            x, y = depth_map.calibration.project(cam.calibration.unproject(Q))

            # Check to make sure it's still within the view of the camera
            if x < 0 or x >= WIDTH or y < 0 or y >= HEIGHT:
                continue

            # It is in the view of the camera; now grab the depth value for
            # the pixel index representing the point we reprojected into the
            # scene (Q/q)
            d = depth[int(x), int(y)][0]

            # Depth maps can be sparse and contain zero values; if the value
            # isn't 0, then we have a proper depth value we can use to
            # determine if P/p is foreground or background. If the value is
            # 0, we need another way to find where Q/q is within the scene.
            # We'll use pickPoint with a low resolution tiled model (though
            # it's slower).
            if d > 0.0:
                ray = cam.unproject(Q) - cam.center
                ray /= ray.norm()
                q = cam.center + (d / cam.transform.inv().mulv(ray).z) * ray

            else:
                # Find the point within in the scene, if it's not in the scene
                # then we wasted our time, skip to next index.
                q = tiled.pickPoint(cam.center, cam.unproject(Q))

            if q is None:
                continue

            # If determine the distance between our supposed point p and
            # the current camera, and the distance between the actual point
            # q and the current camera.
            p_dist = np.linalg.norm(cam.center - p)
            q_dist = np.linalg.norm(cam.center - q)

            # If p is significantly different from q, then we know that p is in
            # the background and obstructed from view, so don't include it.
            if np.abs(q_dist - p_dist) > 0.1:
                continue

            # Else, we have a point, add the linked camera to the list
            camera_label = cam.label.split(".")[0]
            linked_camera = np.array([camera_label, u, v, R, G, B])

            if linked_camera.shape[0] == 6:
                linked_cameras.append(linked_camera)

    return linked_cameras


def mt_find_linked_images(indices, m_dict):
    """This function takes in a list of indices and uses ThreadPool to run
    the find_linked_images function multiple times simultaneously. Indices
    provided are a subset of the total amount, as memory consumption would
    be too high otherwise. Function returns a np array containing the
    following information for each labeled point in the orthomosaic: linked
    image name, x and y coordinate in image space, RGB color label value."""

    batches = np.array_split(indices, CORES if MULTI else 1)

    # Creating a Threadpool
    p = TPool(CORES)

    # Annotations contains a list of lists of linked camera dicts;
    annotations = p.map(partial(find_linked_images, m_dict=m_dict), batches)
    p.close()

    # Some lists returned may be empty, so remove those before vstacking.
    annotations = np.array([a for a in annotations if a != []])

    if len(annotations) > 0:
        annotations = np.vstack(annotations)

    return annotations


def sample_ortho(ortho, frac=0.75):
    """This function takes in the labeled orthomosaic and returns a sample of
    y, x, indices and their corresponding labels."""

    y_colors, x_colors = np.where(np.any(ortho != [0, 0, 0], axis=-1))

    # Given the samples, randomly sample. Vary this frac value as needed.
    num_indices = len(list(zip(y_colors, x_colors)))
    sample = int(num_indices * frac)
    i = np.random.default_rng().choice(np.arange(0, num_indices), sample)
    y_colors, x_colors = y_colors[i], x_colors[i]
    l_colors = ortho[y_colors, x_colors]

    indices = np.column_stack((y_colors, x_colors, l_colors))

    print("Sampled Orthomosaic: ", len(indices))

    return indices


# -----------------------------------------------------------------------------
# Configurations
# -----------------------------------------------------------------------------
# Setting the H and W of the source images
# Might change this in the future, images may be different dimensions.
HEIGHT, WIDTH = 1619, 2159
CORES = 12
MULTI = False

cwd = "C:/Users/jordan.pierce/Documents/GitHub/SfM_Back_Projection"
doc_path = cwd + "/Project/back_project.psx"
ortho_path = cwd + "/Project/Products/orthomosaic_labeled.tif"
image_dir = cwd + "/Project/Images/Tif/"
depth_dir = cwd + "/Project/Images/Depth/"
mask_dir = cwd + "/Project/Images/Mask/"
sparse_dir = cwd + "/Project/Images/Sparse/"
dense_dir = cwd + "/Project/Images/Dense/"

# Set the Metashape document, open it.
doc = M.Document()
doc.open(doc_path, True, True)

# Open the orthomosaic (HxWxC), ignore alpha if provided
print("Opening Orthomosaic...")
ortho = imread(ortho_path)[:, :, 0:3]
print("Orthomosaic shape: ", ortho.shape)

# Ask where the images are
prompt = "Enter the Directory Containing Images: "
# image_dir = M.app.getExistingDirectory(prompt) + "/"

# Ask where to place the labeled images
prompt = "Enter the Directory To Contain Labeled Images: "
# output_dir = M.app.getExistingDirectory(prompt) + "/"

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

if __name__ == '__main__':

    t0 = time.time()

    # Make the folders to contain temporary npy files
    sparse_paths = make_sparse_label_folders(image_dir, sparse_dir)

    # Output the depth maps now for all cameras
    depth_maps, depths = make_depth_maps(depth_dir)

    # Obtain a sample of indices from the orthomosaic along with labels.
    indices = sample_ortho(ortho, 1)

    # A dict holding shared information from metashape.
    m_dict = {'chunk': doc.chunk,
              'ortho': doc.chunk.orthomosaic,
              'dem': doc.chunk.elevation,
              'tiled': doc.chunk.tiled_model,
              'depth_maps': depth_maps,
              'depths': depths,
              'T': doc.chunk.transform.matrix}

    annotations = mt_find_linked_images(indices, m_dict)
    annotation_paths = mt_save_annotations(annotations)
    sparse_paths = mt_construct_sparse_mask(sparse_dir, mask_dir)

    # print("Total N Annotations: ", len(annotations))
    print(time.time() - t0)
    print("Done.")